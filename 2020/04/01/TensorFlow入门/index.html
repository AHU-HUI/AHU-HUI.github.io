<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    Tensorflow Usage |  HUI&#39;s Blog
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

</head>

</html>

<body>
  <div id="app">
    <main class="content">
      <section class="outer">
  <article id="post-TensorFlow入门" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  Tensorflow Usage
</h1>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/04/01/TensorFlow%E5%85%A5%E9%97%A8/" class="article-date">
  <time datetime="2020-04-01T07:35:32.000Z" itemprop="datePublished">2020-04-01</time>
</a>
      
      
      
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">6.5k字</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">27分钟</span>
        </span>
    </span>
</div>

      
    </div>
    

    
    
    <div class="tocbot"></div>





    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="一、Model类"><a href="#一、Model类" class="headerlink" title="一、Model类"></a>一、Model类</h1><h4 id="（1）初始化方法"><a href="#（1）初始化方法" class="headerlink" title="（1）初始化方法"></a>（1）初始化方法</h4><p><strong>1.可扩展参数：</strong>从<code>kwargs</code>字典里获取，可限制key的取值</p>
<ul>
<li><code>name</code>: <code>model</code>变量域<code>scope</code>名称，获取不到采用<code>self.__class__.__name__.lower()</code>取类的名字</li>
<li><code>logging</code>: 是否开启<code>TensorBoard</code>直方图仪表板，获取不到为<code>False</code><a id="more"></a></li>
</ul>
<ol>
<li>其他参数</li>
</ol>
<p><code>vars</code>: <code>name scope</code>下的变量（字典）<br><code>placeholders</code>: 外部变量占位，一般是特征和标签（字典）<br><code>layers</code>： 神经网络的<code>layer</code>（列表）<br><code>activations</code>： 每个<code>layer</code>的输出结果（列表）<br><code>inputs</code>： 输入<br><code>output</code>： 输出<br><code>loss</code>： 损失<br><code>accuracy</code>： 准确率<br><code>optimizer</code>：优化器<br><code>opt_op</code>：最优化的<code>op</code>操作<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, **kwargs):</span><br><span class="line">    allowed_kwargs &#x3D; &#123;&#39;name&#39;, &#39;logging&#39;&#125;</span><br><span class="line">    for kwarg in kwargs.keys():</span><br><span class="line">        assert kwarg in allowed_kwargs, &#39;Invalid keyword argument: &#39; + kwarg</span><br><span class="line">    name &#x3D; kwargs.get(&#39;name&#39;)</span><br><span class="line">    if not name:</span><br><span class="line">        name &#x3D; self.__class__.__name__.lower()</span><br><span class="line">    self.name &#x3D; name</span><br><span class="line"></span><br><span class="line">    logging &#x3D; kwargs.get(&#39;logging&#39;, False)</span><br><span class="line">    self.logging &#x3D; logging</span><br><span class="line"></span><br><span class="line">    self.vars &#x3D; &#123;&#125;</span><br><span class="line">    self.placeholders &#x3D; &#123;&#125;</span><br><span class="line"></span><br><span class="line">    self.layers &#x3D; []</span><br><span class="line">    self.activations &#x3D; []</span><br><span class="line"></span><br><span class="line">    self.inputs &#x3D; None</span><br><span class="line">    self.outputs &#x3D; None</span><br><span class="line"></span><br><span class="line">    self.loss &#x3D; 0</span><br><span class="line">    self.accuracy &#x3D; 0</span><br><span class="line">    self.optimizer &#x3D; None</span><br><span class="line">    self.opt_op &#x3D; None_build </span><br></pre></td></tr></table></figure></p>
<h4 id="（2）build方法"><a href="#（2）build方法" class="headerlink" title="（2）build方法"></a>（2）<code>build</code>方法</h4><p><code>_build</code> 是私有<code>build</code>方法，在继承Model的具体实现时对<code>layers</code>进行<code>append</code>操作，下面介绍<code>build</code>方法：</p>
<p>调用<code>_build</code>，所有变量设置共享空间（<code>self.name</code>）<br>构建模型序列：给输入，通过layer()返回输出，又将这个输出再次作为输入到下一个<code>layer()</code>中，循环这一过程；最终，取最后一层<code>layer</code>的结果作为<code>output</code><br>保存<code>name scope下</code>的变量到<code>self.vars</code><br>模型效果度量：<code>_loss</code>方法，<code>_accuracy</code>方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def _build(self):</span><br><span class="line">    raise NotImplementedError</span><br><span class="line"></span><br><span class="line">def build(self):</span><br><span class="line">    &quot;&quot;&quot; Wrapper for _build() &quot;&quot;&quot;</span><br><span class="line">    with tf.variable_scope(self.name):</span><br><span class="line">        self._build()</span><br><span class="line"></span><br><span class="line">    # Build sequential layer model</span><br><span class="line">    self.activations.append(self.inputs)</span><br><span class="line">    for layer in self.layers:</span><br><span class="line">        hidden &#x3D; layer(self.activations[-1])</span><br><span class="line">        self.activations.append(hidden)</span><br><span class="line">    self.outputs &#x3D; self.activations[-1]</span><br><span class="line"></span><br><span class="line">    # Store model variables for easy access</span><br><span class="line">    variables &#x3D; tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope&#x3D;self.name)</span><br><span class="line">    self.vars &#x3D; &#123;var.name: var for var in variables&#125;</span><br><span class="line"></span><br><span class="line">    # Build metrics</span><br><span class="line">    self._loss()</span><br><span class="line">    self._accuracy()</span><br><span class="line"></span><br><span class="line">    self.opt_op &#x3D; self.optimizer.minimize(self.loss)</span><br></pre></td></tr></table></figure></p>
<h4 id="（3）保存与加载方法"><a href="#（3）保存与加载方法" class="headerlink" title="（3）保存与加载方法"></a>（3）保存与加载方法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def save(self, sess&#x3D;None):</span><br><span class="line">        if not sess:</span><br><span class="line">            raise AttributeError(&quot;TensorFlow session not provided.&quot;)</span><br><span class="line">        saver &#x3D; tf.train.Saver(self.vars)</span><br><span class="line">        save_path &#x3D; saver.save(sess, &quot;tmp&#x2F;%s.ckpt&quot; % self.name)</span><br><span class="line">        print(&quot;Model saved in file: %s&quot; % save_path)</span><br><span class="line"></span><br><span class="line">def load(self, sess&#x3D;None):</span><br><span class="line">        if not sess:</span><br><span class="line">            raise AttributeError(&quot;TensorFlow session not provided.&quot;)</span><br><span class="line">        saver &#x3D; tf.train.Saver(self.vars)</span><br><span class="line">        save_path &#x3D; &quot;tmp&#x2F;%s.ckpt&quot; % self.name</span><br><span class="line">        saver.restore(sess, save_path)</span><br><span class="line">        print(&quot;Model restored from file: %s&quot; % save_path)    </span><br></pre></td></tr></table></figure>
<h1 id="二、数据读取"><a href="#二、数据读取" class="headerlink" title="二、数据读取"></a>二、数据读取</h1><p>常用的结构化数据文件格式有csv、txt 、libsvm，本篇文章主要说明结构化数据（csv/txt）如何在TF框架进行高效、灵活的读取，避免一些不合理的方式，迈出算法开发标准化、工程化的第一步。</p>
<h4 id="使用pandas读取数据"><a href="#使用pandas读取数据" class="headerlink" title="使用pandas读取数据"></a>使用<code>pandas</code>读取数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def load_data_with_pandas(filename):</span><br><span class="line">    # load data as DataFrame to the memory</span><br><span class="line">    data &#x3D; pd.read_csv(filename, sep&#x3D;&#39;,&#39;, header&#x3D;0)</span><br><span class="line">    features_df &#x3D; data.iloc[:, :-1]</span><br><span class="line">    label_df &#x3D; data.iloc[:, -1]</span><br><span class="line">    features &#x3D; features_df.astype(np.float32).values</span><br><span class="line">    labels &#x3D; label_df.astype(np.int64).values</span><br><span class="line">    return features, labels</span><br></pre></td></tr></table></figure>
<p>最常见的读取数据的方式是利用pandas包将csv、txt读取为DataFrame，一次全部放入内存。这是一种非常低效的方式，应该尽量避免这种读取方法。其他第三方封装python读取方式的包（TFLearn等）也不建议使用，推荐使用TF框架的OP操作进行数据读取。</p>
<blockquote>
<h2 id="TensorFlow的数据读取机制"><a href="#TensorFlow的数据读取机制" class="headerlink" title="TensorFlow的数据读取机制"></a>TensorFlow的数据读取机制</h2><h4 id="一、tensorflow读取机制图解"><a href="#一、tensorflow读取机制图解" class="headerlink" title="一、tensorflow读取机制图解"></a>一、tensorflow读取机制图解</h4><p>首先需要思考的一个问题是，什么是数据读取？以图像数据为例，读取数据的过程可以用下图来表示：<br><img src="/images/5868203-b6583102740cb1c8.png" alt=""><br>假设我们的硬盘中有一个图片数据集0001.jpg，0002.jpg，0003.jpg……我们只需要把它们读取到内存中，然后提供给GPU或是CPU进行计算就可以了。这听起来很容易，但事实远没有那么简单。<strong>事实上，我们必须要把数据先读入后才能进行计算，假设读入用时0.1s，计算用时0.9s，那么就意味着每过1s，GPU都会有0.1s无事可做，这就大大降低了运算的效率。</strong></p>
<p>如何解决这个问题？方法就是将读入数据和计算分别放在两个线程中，将数据读入内存的一个队列，如下图所示：<br><img src="/images/5868203-d27d786f72729714.png" alt=""><br><strong>读取线程源源不断地将文件系统中的图片读入到一个内存的队列中，而负责计算的是另一个线程，计算需要数据时，直接从内存队列中取就可以了。这样就可以解决GPU因为IO而空闲的问题！</strong></p>
<p>而在tensorflow中，为了方便管理，在内存队列前又添加了一层所谓的<strong>“文件名队列”</strong>。</p>
<p>为什么要添加这一层文件名队列？我们首先得了解机器学习中的一个概念：epoch。对于一个数据集来讲，运行一个epoch就是将这个数据集中的图片全部计算一遍。如一个数据集中有三张图片A.jpg、B.jpg、C.jpg，那么跑一个epoch就是指对A、B、C三张图片都计算了一遍。两个epoch就是指先对A、B、C各计算一遍，然后再全部计算一遍，也就是说每张图片都计算了两遍。</p>
<p><strong>tensorflow使用文件名队列+内存队列双队列</strong>的形式读入文件，可以很好地管理epoch。下面我们用图片的形式来说明这个机制的运行方式。如下图，还是以数据集A.jpg, B.jpg, C.jpg为例，假定我们要跑一个epoch，那么我们就在文件名队列中把A、B、C各放入一次，并在之后标注队列结束。<br><img src="/images/5868203-330d03d927baef27.png" alt=""></p>
<p>程序运行后，内存队列首先读入A（此时A从文件名队列中出队）：<img src="/images/5868203-000e79f5d13b82dc.png" alt=""></p>
<p>再依次读入B和C：<br><img src="/images/5868203-7c8dbc2f81cbef53.png" alt=""></p>
<p><img src="/images/5868203-d1fb5adc86281ad9.png" alt=""></p>
<p>此时，如果再尝试读入，系统由于检测到了“结束”，就会自动抛出一个异常（OutOfRange）。外部捕捉到这个异常后就可以结束程序了。这就是tensorflow中读取数据的基本机制。如果我们要跑2个epoch而不是1个epoch，那只要在文件名队列中将A、B、C依次放入两次再标记结束就可以了。</p>
<h4 id="二、tensorflow读取数据机制的对应函数"><a href="#二、tensorflow读取数据机制的对应函数" class="headerlink" title="二、tensorflow读取数据机制的对应函数"></a>二、tensorflow读取数据机制的对应函数</h4><p>如何在tensorflow中创建上述的两个队列呢？</p>
<p>对于文件名队列，我们使用<code>tf.train.string_input_producer</code>函数。这个函数需要传入一个文件名list，系统会自动将它转为一个文件名队列。</p>
<p>此外<code>tf.train.string_input_producer</code>还有两个重要的参数，一个是num_epochs，它就是我们上文中提到的epoch数。另外一个就是shuffle，shuffle是指在一个epoch内文件的顺序是否被打乱。若设置shuffle=False，如下图，每个epoch内，数据还是按照A、B、C的顺序进入文件名队列，这个顺序不会改变：<br><img src="/images/5868203-36d995ccaf378d1b.png" alt=""><br>如果设置shuffle=True，那么在一个epoch内，数据的前后顺序就会被打乱，如下图所示：<br><img src="/images/5868203-1f176c5cdfdf318d.png" alt=""><br>在tensorflow中，内存队列不需要我们自己建立，我们只需要使用reader对象从文件名队列中读取数据就可以了，具体实现可以参考下面的实战代码。</p>
<p>除了<code>tf.train.string_input_producer</code>外，我们还要额外介绍一个函数：<code>tf.train.start_queue_runners</code>。初学者会经常在代码中看到这个函数，但往往很难理解它的用处，在这里，有了上面的铺垫后，我们就可以解释这个函数的作用了。</p>
<p>在我们使用<code>tf.train.string_input_producer</code>创建文件名队列后，整个系统其实还是处于“停滞状态”的，也就是说，我们文件名并没有真正被加入到队列中（如下图所示）。此时如果我们开始计算，因为内存队列中什么也没有，计算单元就会一直等待，导致整个系统被阻塞。</p>
<p><img src="/images/5868203-614f665f56927dee.png" alt=""></p>
<p>而使用<code>tf.train.start_queue_runners</code>之后，才会启动填充队列的线程，这时系统就不再“停滞”。此后计算单元就可以拿到数据并进行计算，整个程序也就跑起来了，这就是函数<code>tf.train.start_queue_runners</code>的用处。</p>
<p><img src="/images/5868203-6d5e261a1327754c.png" alt=""></p>
<h4 id="三、实战代码"><a href="#三、实战代码" class="headerlink" title="三、实战代码"></a>三、实战代码</h4><p>我们用一个具体的例子感受tensorflow中的数据读取。如图，假设我们在当前文件夹中已经有A.jpg、B.jpg、C.jpg三张图片，我们希望读取这三张图片5个epoch并且把读取的结果重新存到read文件夹中。<br><img src="/images/5868203-60fd12655cd1b5dd.png" alt=""></p>
<p>对应的代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&gt;# 导入tensorflow</span><br><span class="line">&gt;import tensorflow as tf </span><br><span class="line"></span><br><span class="line">&gt;# 新建一个Session</span><br><span class="line">&gt;with tf.Session() as sess:</span><br><span class="line">   # 我们要读三幅图片A.jpg, B.jpg, C.jpg</span><br><span class="line">  filename &#x3D; [&#39;A.jpg&#39;, &#39;B.jpg&#39;, &#39;C.jpg&#39;]</span><br><span class="line">   # string_input_producer会产生一个文件名队列</span><br><span class="line">   filename_queue &#x3D; tf.train.string_input_producer(filename, shuffle&#x3D;False, num_epochs&#x3D;5)</span><br><span class="line">   # reader从文件名队列中读数据。对应的方法是reader.read</span><br><span class="line">   reader &#x3D; tf.WholeFileReader()</span><br><span class="line">   key, value &#x3D; reader.read(filename_queue)</span><br><span class="line">   # tf.train.string_input_producer定义了一个epoch变量，要对它进行初始化</span><br><span class="line">   tf.local_variables_initializer().run()</span><br><span class="line">   # 使用start_queue_runners之后，才会开始填充队列</span><br><span class="line">   threads &#x3D; tf.train.start_queue_runners(sess&#x3D;sess)</span><br><span class="line">   i &#x3D; 0</span><br><span class="line">   while True:</span><br><span class="line">       i +&#x3D; 1</span><br><span class="line">       # 获取图片数据并保存</span><br><span class="line">       image_data &#x3D; sess.run(value)</span><br><span class="line">       with open(&#39;read&#x2F;test_%d.jpg&#39; % i, &#39;wb&#39;) as f:</span><br><span class="line">           f.write(image_data)</span><br></pre></td></tr></table></figure>
<p>我们这里使用<code>filename_queue = tf.train.string_input_producer(filename, shuffle=False, num_epochs=5)</code>建立了一个会跑5个epoch的文件名队列。并使用reader读取，reader每次读取一张图片并保存。<br>运行代码后，我们得到就可以看到read文件夹中的图片，正好是按顺序的5个epoch：<br><img src="/images/5868203-f608599b3d35638e.png" alt=""><br>如果我们设置<code>filename_queue = tf.train.string_input_producer(filename, shuffle=False, num_epochs=5)</code>中的shuffle=True，那么在每个epoch内图像就会被打乱，如图所示：<br><img src="/images/5868203-767838539c804df7.png" alt=""><br><strong>这里只是用三张图片举例，实际应用中一个数据集肯定不止3张图片，不过涉及到的原理都是共通的。</strong></p>
</blockquote>
<p><strong>高效的 TensorFlow 读取方式是将数据读取转换成 OP，通过 <code>session run</code>的方式拉去数据。读取线程源源不断地将文件系统中的文件读入到一个内存的队列中，而负责计算的是另一个线程，计算需要数据时，直接从内存队列中取就可以了，这样就可以解决GPU因为IO而空闲的问题。同时，不会一次性的preload到内存，再大的数据量也不会超出内存的限制。</strong></p>
<h1 id="三、TensorFlow-API"><a href="#三、TensorFlow-API" class="headerlink" title="三、TensorFlow API"></a>三、TensorFlow API</h1><h3 id="1-TensorFlow-相关函数理解"><a href="#1-TensorFlow-相关函数理解" class="headerlink" title="1.TensorFlow 相关函数理解"></a>1.TensorFlow 相关函数理解</h3><hr>
<ul>
<li><strong>tf.nn.conv2d ————卷积</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">conv2d(</span><br><span class="line">    input,</span><br><span class="line">    filter,</span><br><span class="line">    strides,</span><br><span class="line">    padding,</span><br><span class="line">    use_cudnn_on_gpu&#x3D;True,</span><br><span class="line">    data_format&#x3D;&#39;NHWC&#39;,</span><br><span class="line">    name&#x3D;None</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">#Example</span><br><span class="line">import tensorflow as tf</span><br><span class="line">a &#x3D; tf.constant([1,1,1,0,0,0,1,1,1,0,0,0,1,1,1,0,0,1,1,0,0,1,1,0,0],dtype&#x3D;tf.float32,shape&#x3D;[1,5,5,1])</span><br><span class="line">b &#x3D; tf.constant([1,0,1,0,1,0,1,0,1],dtype&#x3D;tf.float32,shape&#x3D;[3,3,1,1])</span><br><span class="line">c &#x3D; tf.nn.conv2d(a,b,strides&#x3D;[1, 2, 2, 1],padding&#x3D;&#39;VALID&#39;)</span><br><span class="line">d &#x3D; tf.nn.conv2d(a,b,strides&#x3D;[1, 2, 2, 1],padding&#x3D;&#39;SAME&#39;)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print (&quot;c shape:&quot;)</span><br><span class="line">    print (c.shape)</span><br><span class="line">    print (&quot;c value:&quot;)</span><br><span class="line">    print (sess.run(c))</span><br><span class="line">    print (&quot;d shape:&quot;)</span><br><span class="line">    print (d.shape)</span><br><span class="line">    print (&quot;d value:&quot;)</span><br><span class="line">    print (sess.run(d))</span><br></pre></td></tr></table></figure>
|参数名|    必选|    类型|    说明|<br>|:—-:|:—-:|:—-:|:—-:|<br>|input|    是|    tensor|    是一个 4 维的 tensor，即 [ batch, in_height, in_width, in_channels ]（若 input 是图像，[ 训练时一个 batch 的图片数量, 图片高度, 图片宽度, 图像通道数 ]）|<br>|filter    |是    |tensor    |是一个 4 维的 tensor，即 [ filter_height, filter_width, in_channels, out_channels ]（若 input 是图像，[ 卷积核的高度，卷积核的宽度，图像通道数，卷积核个数 ]）,filter 的 in_channels 必须和 input 的 in_channels 相等|<br>|strides|    是    |列表    |长度为 4 的 list，卷积时候在 input 上每一维的步长，一般 strides[0] = strides[3] = 1|<br>|padding|    是    |string    |只能为 “ VALID “，” SAME “ 中之一，这个值决定了不同的卷积方式。VALID 丢弃方式；SAME：补全方式|<br>|use_cudnn_on_gpu|    否    |bool    |是否使用 cudnn 加速，默认为 true|<br>|data_format|    否|    string    |只能是 “ NHWC “, “ NCHW “，默认 “ NHWC “|<br>|name|    否    |string|    运算名称|</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/5868203-569d892b36922035.gif?imageMogr2/auto-orient/strip" alt="image"></p>
<hr>
<ul>
<li><strong>tf.nn.relu ————relu激活函数</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">relu(</span><br><span class="line">    features,</span><br><span class="line">    name&#x3D;None</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">#Example</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a &#x3D; tf.constant([1,-2,0,4,-5,6])</span><br><span class="line">b &#x3D; tf.nn.relu(a)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print (sess.run(b))</span><br></pre></td></tr></table></figure>
|参数名|    必选    |类型|    说明|<br>|:—-:|:—-:|:—-:|:—-:|<br>|features|    是|    tensor|    是以下类型float32, float64, int32, int64, uint8, int16, int8, uint16, half|<br>|name|    否|    string|    运算名称|</li>
</ul>
<hr>
<ul>
<li><strong>tf.nn.max_pool————池化</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">max_pool(</span><br><span class="line">    value,</span><br><span class="line">    ksize,</span><br><span class="line">    strides,</span><br><span class="line">    padding,</span><br><span class="line">    data_format&#x3D;&#39;NHWC&#39;,</span><br><span class="line">    name&#x3D;None</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Example</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a &#x3D; tf.constant([1,3,2,1,2,9,1,1,1,3,2,3,5,6,1,2],dtype&#x3D;tf.float32,shape&#x3D;[1,4,4,1])</span><br><span class="line">b &#x3D; tf.nn.max_pool(a,ksize&#x3D;[1, 2, 2, 1],strides&#x3D;[1, 2, 2, 1],padding&#x3D;&#39;VALID&#39;)</span><br><span class="line">c &#x3D; tf.nn.max_pool(a,ksize&#x3D;[1, 2, 2, 1],strides&#x3D;[1, 2, 2, 1],padding&#x3D;&#39;SAME&#39;)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print (&quot;b shape:&quot;)</span><br><span class="line">    print (b.shape)</span><br><span class="line">    print (&quot;b value:&quot;)</span><br><span class="line">    print (sess.run(b))</span><br><span class="line">    print (&quot;c shape:&quot;)</span><br><span class="line">    print (c.shape)</span><br><span class="line">    print (&quot;c value:&quot;)</span><br><span class="line">    print (sess.run(c))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数名</th>
<th style="text-align:center">必选</th>
<th style="text-align:center">类型</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">value</td>
<td style="text-align:center">是</td>
<td style="text-align:center">tensor</td>
<td style="text-align:center">4 维的张量，即 [ batch, height, width, channels ]，数据类型为 tf.float32</td>
</tr>
<tr>
<td style="text-align:center">ksize</td>
<td style="text-align:center">是</td>
<td style="text-align:center">列表</td>
<td style="text-align:center">池化窗口的大小，长度为 4 的 list，一般是 [1, height, width, 1]，因为不在 batch 和 channels 上做池化，所以第一个和最后一个维度为 1</td>
</tr>
<tr>
<td style="text-align:center">strides</td>
<td style="text-align:center">是</td>
<td style="text-align:center">列表</td>
<td style="text-align:center">池化窗口在每一个维度上的步长，一般 strides[0] = strides[3] = 1</td>
</tr>
<tr>
<td style="text-align:center">padding</td>
<td style="text-align:center">是</td>
<td style="text-align:center">string</td>
<td style="text-align:center">只能为 “ VALID “，” SAME “ 中之一，这个值决定了不同的池化方式。VALID 丢弃方式；SAME：补全方式</td>
</tr>
<tr>
<td style="text-align:center">data_format</td>
<td style="text-align:center">否</td>
<td style="text-align:center">string</td>
<td style="text-align:center">只能是 “ NHWC “, “ NCHW “，默认” NHWC “</td>
</tr>
<tr>
<td style="text-align:center">name</td>
<td style="text-align:center">否</td>
<td style="text-align:center">string</td>
<td style="text-align:center">运算名称</td>
</tr>
</tbody>
</table>
</div>
<hr>
<ul>
<li><strong>tf.nn.dropout————防止或减轻过拟合</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">dropout(</span><br><span class="line">    x,</span><br><span class="line">    keep_prob,</span><br><span class="line">    noise_shape&#x3D;None,</span><br><span class="line">    seed&#x3D;None,</span><br><span class="line">    name&#x3D;None</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">#Example</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a &#x3D; tf.constant([1,2,3,4,5,6],shape&#x3D;[2,3],dtype&#x3D;tf.float32)</span><br><span class="line">b &#x3D; tf.placeholder(tf.float32)</span><br><span class="line">c &#x3D; tf.nn.dropout(a,b,[2,1],1)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print (sess.run(c,feed_dict&#x3D;&#123;b:0.75&#125;))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数名</th>
<th style="text-align:center">必选</th>
<th style="text-align:center">类型</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">x</td>
<td style="text-align:center">是</td>
<td style="text-align:center">tensor</td>
<td style="text-align:center">输出元素是 x 中的元素以 keep_prob 概率除以 keep_prob，否则为 0</td>
</tr>
<tr>
<td style="text-align:center">keep_prob</td>
<td style="text-align:center">是</td>
<td style="text-align:center">scalar Tensor</td>
<td style="text-align:center">dropout 的概率，一般是占位符</td>
</tr>
<tr>
<td style="text-align:center">noise_shape</td>
<td style="text-align:center">否</td>
<td style="text-align:center">tensor</td>
<td style="text-align:center">默认情况下，每个元素是否 dropout 是相互独立。如果指定 noise_shape，若 noise_shape[i] == shape(x)[i]，该维度的元素是否 dropout 是相互独立，若 noise_shape[i] != shape(x)[i] 该维度元素是否 dropout 不相互独立，要么一起 dropout 要么一起保留</td>
</tr>
<tr>
<td style="text-align:center">seed</td>
<td style="text-align:center">否</td>
<td style="text-align:center">数值</td>
<td style="text-align:center">如果指定该值，每次 dropout 结果相同</td>
</tr>
<tr>
<td style="text-align:center">name</td>
<td style="text-align:center">否</td>
<td style="text-align:center">string</td>
<td style="text-align:center">运算名称</td>
</tr>
</tbody>
</table>
</div>
<hr>
<ul>
<li><strong>tf.nn.sigmoid_cross_entropy_with_logits————先对 logits 通过 sigmoid 计算，再计算交叉熵</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sigmoid_cross_entropy_with_logits(</span><br><span class="line">    _sentinel&#x3D;None,</span><br><span class="line">    labels&#x3D;None,</span><br><span class="line">    logits&#x3D;None,</span><br><span class="line">    name&#x3D;None</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">#Example</span><br><span class="line">import tensorflow as tf</span><br><span class="line">x &#x3D; tf.constant([1,2,3,4,5,6,7],dtype&#x3D;tf.float64)</span><br><span class="line">y &#x3D; tf.constant([1,1,1,0,0,1,0],dtype&#x3D;tf.float64)</span><br><span class="line">loss &#x3D; tf.nn.sigmoid_cross_entropy_with_logits(labels &#x3D; y,logits &#x3D; x)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print (sess.run(loss))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数名</th>
<th style="text-align:center">必选</th>
<th style="text-align:center">类型</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">_sentinel</td>
<td style="text-align:center">否</td>
<td style="text-align:center">None</td>
<td style="text-align:center">没有使用的参数</td>
</tr>
<tr>
<td style="text-align:center">labels</td>
<td style="text-align:center">否</td>
<td style="text-align:center">Tensor</td>
<td style="text-align:center">type, shape 与 logits相同</td>
</tr>
<tr>
<td style="text-align:center">logits</td>
<td style="text-align:center">否</td>
<td style="text-align:center">Tensor</td>
<td style="text-align:center">type 是 float32 或者 float64</td>
</tr>
<tr>
<td style="text-align:center">name</td>
<td style="text-align:center">否</td>
<td style="text-align:center">string</td>
<td style="text-align:center">运算名称</td>
</tr>
</tbody>
</table>
</div>
<hr>
<ul>
<li><strong>tf.truncated_normal————产生截断正态分布随机数</strong>，取值范围为 [ mean - 2 <em> stddev, mean + 2 </em> stddev ]<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">truncated_normal(</span><br><span class="line">    shape,</span><br><span class="line">    mean&#x3D;0.0,</span><br><span class="line">    stddev&#x3D;1.0,</span><br><span class="line">    dtype&#x3D;tf.float32,</span><br><span class="line">    seed&#x3D;None,</span><br><span class="line">    name&#x3D;None</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">#Example</span><br><span class="line">import tensorflow as tf</span><br><span class="line">initial &#x3D; tf.truncated_normal(shape&#x3D;[3,3], mean&#x3D;0, stddev&#x3D;1)</span><br><span class="line">print(tf.Session().run(initial))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数名</th>
<th style="text-align:center">必选</th>
<th style="text-align:center">类型</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">shape</td>
<td style="text-align:center">是</td>
<td style="text-align:center">1 维整形张量或 array</td>
<td style="text-align:center">输出张量的维度</td>
</tr>
<tr>
<td style="text-align:center">mean</td>
<td style="text-align:center">否</td>
<td style="text-align:center">0 维张量或数值</td>
<td style="text-align:center">均值</td>
</tr>
<tr>
<td style="text-align:center">stddev</td>
<td style="text-align:center">否</td>
<td style="text-align:center">0 维张量或数值</td>
<td style="text-align:center">标准差</td>
</tr>
<tr>
<td style="text-align:center">dtype</td>
<td style="text-align:center">否</td>
<td style="text-align:center">dtype</td>
<td style="text-align:center">输出类型</td>
</tr>
<tr>
<td style="text-align:center">seed</td>
<td style="text-align:center">否</td>
<td style="text-align:center">数值</td>
<td style="text-align:center">随机种子，若 seed 赋值，每次产生相同随机数</td>
</tr>
<tr>
<td style="text-align:center">name</td>
<td style="text-align:center">否</td>
<td style="text-align:center">string</td>
<td style="text-align:center">运算名称</td>
</tr>
</tbody>
</table>
</div>
<hr>
<ul>
<li><strong>tf.constant————根据 value 的值生成一个 shape 维度的常量张量</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">constant(</span><br><span class="line">    value,</span><br><span class="line">    dtype&#x3D;None,</span><br><span class="line">    shape&#x3D;None,</span><br><span class="line">    name&#x3D;&#39;Const&#39;,</span><br><span class="line">    verify_shape&#x3D;False</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
|参数名|    必选|    类型|    说明|<br>|:—-:|:—-:|:—-:|:—-:|<br>|value|    是|    常量数值或者 list|    输出张量的值|<br>|dtype|    否|    dtype    |输出张量元素类型|<br>|shape|    否|    1 维整形张量或 array|    输出张量的维度|<br>|name|    否|    string|    张量名称|<br>|verify_shape|    否    |Boolean|    检测 shape 是否和 value 的 shape 一致，若为 Fasle，不一致时，会用最后一个元素将 shape 补全|</li>
</ul>
<hr>
<ul>
<li><strong>tf.placeholder————是一种占位符，在执行时候需要为其提供数据</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">placeholder(</span><br><span class="line">    dtype,</span><br><span class="line">    shape&#x3D;None,</span><br><span class="line">    name&#x3D;None</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">#Example</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x &#x3D; tf.placeholder(tf.float32,[None,3])</span><br><span class="line">y &#x3D; tf.matmul(x,x)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    rand_array &#x3D; np.random.rand(3,3)</span><br><span class="line">    print(sess.run(y,feed_dict&#x3D;&#123;x:rand_array&#125;))</span><br></pre></td></tr></table></figure>
|参数名|    必选|    类型|    说明|<br>|:—-:|:—-:|:—-:|:—-:|<br>|dtype|    是|    dtype    |占位符数据类型|<br>|shape|    否|1 维整形张量或 array|    占位符维度|<br>|name|    否|    string    |占位符名称|</li>
</ul>
<hr>
<ul>
<li><strong>tf.nn.bias_add————将偏差项 bias 加到 value 上面</strong>，可以看做是 tf.add 的一个特例，其中 bias 必须是一维的，并且维度和 value 的最后一维相同，数据类型必须和 value 相同。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">bias_add(</span><br><span class="line">    value,</span><br><span class="line">    bias,</span><br><span class="line">    data_format&#x3D;None,</span><br><span class="line">    name&#x3D;None</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">#Example</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a &#x3D; tf.constant([[1.0, 2.0],[1.0, 2.0],[1.0, 2.0]])</span><br><span class="line">b &#x3D; tf.constant([2.0,1.0])</span><br><span class="line">c &#x3D; tf.constant([1.0])</span><br><span class="line">sess &#x3D; tf.Session()</span><br><span class="line">print (sess.run(tf.nn.bias_add(a, b)))</span><br><span class="line">#print (sess.run(tf.nn.bias_add(a,c))) error</span><br><span class="line">print (&quot;##################################&quot;)</span><br><span class="line">print (sess.run(tf.add(a, b)))</span><br><span class="line">print (&quot;##################################&quot;)</span><br><span class="line">print (sess.run(tf.add(a, c)))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数名</th>
<th style="text-align:center">必选</th>
<th style="text-align:center">类型</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">value</td>
<td style="text-align:center">是</td>
<td style="text-align:center">张量</td>
<td style="text-align:center">数据类型为 float, double, int64, int32, uint8, int16, int8, complex64, or complex128</td>
</tr>
<tr>
<td style="text-align:center">bias</td>
<td style="text-align:center">是</td>
<td style="text-align:center">一维张量</td>
<td style="text-align:center">维度必须和 value 最后一维维度相等</td>
</tr>
<tr>
<td style="text-align:center">data_format</td>
<td style="text-align:center">否</td>
<td style="text-align:center">string</td>
<td style="text-align:center">数据格式，支持 ‘ NHWC ‘ 和 ‘ NCHW ‘</td>
</tr>
<tr>
<td style="text-align:center">name</td>
<td style="text-align:center">否</td>
<td style="text-align:center">string</td>
<td style="text-align:center">运算名称</td>
</tr>
</tbody>
</table>
</div>
<hr>
<ul>
<li><strong>tf.reduce_mean————计算张量 input_tensor 平均值</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">reduce_mean(</span><br><span class="line">    input_tensor,</span><br><span class="line">    axis&#x3D;None,</span><br><span class="line">    keep_dims&#x3D;False,</span><br><span class="line">    name&#x3D;None,</span><br><span class="line">    reduction_indices&#x3D;None</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">#Example</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">initial &#x3D; [[1.,1.],[2.,2.]]</span><br><span class="line">x &#x3D; tf.Variable(initial,dtype&#x3D;tf.float32)</span><br><span class="line">init_op &#x3D; tf.global_variables_initializer()</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(sess.run(tf.reduce_mean(x)))</span><br><span class="line">    print(sess.run(tf.reduce_mean(x,0))) #Column</span><br><span class="line">    print(sess.run(tf.reduce_mean(x,1))) #row</span><br></pre></td></tr></table></figure>
|参数名|    必选|    类型|    说明|<br>|:—-:|:—-:|:—-:|:—-:|<br>|input_tensor    |是|    张量    输入待求平均值的张量|<br>|axis    |否    |None、0、1    |None：全局求平均值；0：求每一列平均值；1：求每一行平均值|<br>|keep_dims    |否    |Boolean    |保留原来的维度(例如不会从二维矩阵降为一维向量)|<br>|name    |否    |string|    运算名称|<br>|reduction_indices|    否    |None|    和 axis 等价，被弃用|</li>
</ul>
<hr>
<ul>
<li><strong>tf.squared_difference————计算张量 x、y 对应元素差平方</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">squared_difference(</span><br><span class="line">    x,</span><br><span class="line">    y,</span><br><span class="line">    name&#x3D;None</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数名</th>
<th style="text-align:center">必选</th>
<th style="text-align:center">类型</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">x</td>
<td style="text-align:center">是</td>
<td style="text-align:center">张量</td>
<td style="text-align:center">是 half, float32, float64, int32, int64, complex64, complex128 其中一种类型</td>
</tr>
<tr>
<td style="text-align:center">y</td>
<td style="text-align:center">是</td>
<td style="text-align:center">张量</td>
<td style="text-align:center">是 half, float32, float64, int32, int64, complex64, complex128 其中一种类型</td>
</tr>
<tr>
<td style="text-align:center">name</td>
<td style="text-align:center">否</td>
<td style="text-align:center">string</td>
<td style="text-align:center">运算名称</td>
</tr>
</tbody>
</table>
</div>
<hr>
<ul>
<li><strong>tf.square————计算张量对应元素平方</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">square(</span><br><span class="line">    x,</span><br><span class="line">    name&#x3D;None</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数名</th>
<th style="text-align:center">必选</th>
<th style="text-align:center">类型</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">x</td>
<td style="text-align:center">是</td>
<td style="text-align:center">张量</td>
<td style="text-align:center">是half, float32, float64, int32, int64, complex64, complex128 其中一种类型</td>
</tr>
<tr>
<td style="text-align:center">name</td>
<td style="text-align:center">否</td>
<td style="text-align:center">string</td>
<td style="text-align:center">运算名称</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="2-Tensorflow相关类理解"><a href="#2-Tensorflow相关类理解" class="headerlink" title="2.Tensorflow相关类理解"></a>2.Tensorflow相关类理解</h3><ul>
<li>tf.Variable————维护图在执行过程中的状态信息，例如神经网络权重值的变化</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    initial_value&#x3D;None,</span><br><span class="line">    trainable&#x3D;True,</span><br><span class="line">    collections&#x3D;None,</span><br><span class="line">    validate_shape&#x3D;True,</span><br><span class="line">    caching_device&#x3D;None,</span><br><span class="line">    name&#x3D;None,</span><br><span class="line">    variable_def&#x3D;None,</span><br><span class="line">    dtype&#x3D;None,</span><br><span class="line">    expected_shape&#x3D;None,</span><br><span class="line">    import_scope&#x3D;None</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数名</th>
<th style="text-align:center">类型</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">initial_value</td>
<td style="text-align:center">张量</td>
<td style="text-align:center">Variable 类的初始值，这个变量必须指定 shape 信息，否则后面 validate_shape 需设为 False</td>
</tr>
<tr>
<td style="text-align:center">trainable</td>
<td style="text-align:center">Boolean</td>
<td style="text-align:center">是否把变量添加到 collection GraphKeys.TRAINABLE_VARIABLES 中（collection 是一种全局存储，不受变量名生存空间影响，一处保存，到处可取）</td>
</tr>
<tr>
<td style="text-align:center">collections</td>
<td style="text-align:center">Graph collections</td>
<td style="text-align:center">全局存储，默认是 GraphKeys.GLOBAL_VARIABLES</td>
</tr>
<tr>
<td style="text-align:center">validate_shape</td>
<td style="text-align:center">Boolean</td>
<td style="text-align:center">是否允许被未知维度的 initial_value 初始化</td>
</tr>
<tr>
<td style="text-align:center">caching_device</td>
<td style="text-align:center">string</td>
<td style="text-align:center">指明哪个 device 用来缓存变量</td>
</tr>
<tr>
<td style="text-align:center">name</td>
<td style="text-align:center">string</td>
<td style="text-align:center">变量名</td>
</tr>
<tr>
<td style="text-align:center">dtype</td>
<td style="text-align:center">dtype</td>
<td style="text-align:center">如果被设置，初始化的值就会按照这个类型初始化</td>
</tr>
<tr>
<td style="text-align:center">expected_shape</td>
<td style="text-align:center">TensorShape</td>
<td style="text-align:center">要是设置了，那么初始的值会是这种维度</td>
</tr>
</tbody>
</table>
</div>
<h2 id="—"><a href="#—" class="headerlink" title="—-"></a>—-</h2><h1 id="四、LeNet网络的构建"><a href="#四、LeNet网络的构建" class="headerlink" title="四、LeNet网络的构建"></a>四、LeNet网络的构建</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">#模型训练</span><br><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#打开数据集</span><br><span class="line">path&#x3D;&#39;C:&#x2F;Users&#x2F;86769&#x2F;.keras&#x2F;datasets&#x2F;mnist.npz&#39;</span><br><span class="line">f &#x3D; np.load(path)</span><br><span class="line">x_train, y_train &#x3D; f[&#39;x_train&#39;], f[&#39;y_train&#39;]</span><br><span class="line">x_test, y_test &#x3D; f[&#39;x_test&#39;], f[&#39;y_test&#39;]</span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line">#格式化</span><br><span class="line">x_train &#x3D; np.pad(x_train,((0,0),(2,2),(2,2)),&#39;constant&#39;,constant_values&#x3D;0)</span><br><span class="line">x_train &#x3D; x_train.astype(&#39;float32&#39;)</span><br><span class="line">x_train &#x2F;&#x3D;255</span><br><span class="line">x_train&#x3D;x_train.reshape(x_train.shape[0],32,32,1)</span><br><span class="line"></span><br><span class="line">x_test &#x3D; np.pad(x_test,((0,0),(2,2),(2,2)),&#39;constant&#39;,constant_values&#x3D;0)</span><br><span class="line">x_test &#x3D; x_test.astype(&#39;float32&#39;)</span><br><span class="line">x_test &#x2F;&#x3D;255</span><br><span class="line">x_test&#x3D;x_test.reshape(x_test.shape[0],32,32,1)</span><br><span class="line"></span><br><span class="line">#创建模型</span><br><span class="line">model&#x3D;tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Conv2D(filters&#x3D;6,kernel_size&#x3D;(5,5),padding&#x3D;&#39;valid&#39;,activation&#x3D;tf.nn.relu,input_shape&#x3D;(32,32,1)),</span><br><span class="line">    tf.keras.layers.AveragePooling2D(pool_size&#x3D;(2,2),strides&#x3D;(2,2),padding&#x3D;&#39;same&#39;),</span><br><span class="line">    tf.keras.layers.Conv2D(filters&#x3D;16,kernel_size&#x3D;(5,5),padding&#x3D;&#39;valid&#39;,activation&#x3D;tf.nn.relu),</span><br><span class="line">    tf.keras.layers.AveragePooling2D(pool_size&#x3D;(2,2),strides&#x3D;(2,2),padding&#x3D;&#39;same&#39;),</span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(units&#x3D;120,activation&#x3D;tf.nn.relu),</span><br><span class="line">    # tf.keras.layers.Conv2D(filters&#x3D;120,kernel_size&#x3D;(5,5),strides&#x3D;(1,1),activation&#x3D;&#39;tanh&#39;,padding&#x3D;&#39;valid&#39;),</span><br><span class="line">    # tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(units&#x3D;84,activation&#x3D;tf.nn.relu),</span><br><span class="line">    tf.keras.layers.Dense(units&#x3D;10,activation&#x3D;tf.nn.softmax)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#超参数配置</span><br><span class="line">num_epochs&#x3D;10          #训练轮数</span><br><span class="line">batch_size&#x3D;64          #批大小</span><br><span class="line">learning_rate&#x3D;0.001    #学习率</span><br><span class="line"></span><br><span class="line">#优化器</span><br><span class="line">adam_optimizer&#x3D;tf.keras.optimizers.Adam(learning_rate)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer&#x3D;adam_optimizer,</span><br><span class="line">              loss&#x3D;tf.keras.losses.sparse_categorical_crossentropy,metrics&#x3D;[&#39;accuracy&#39;])</span><br><span class="line"></span><br><span class="line">import datetime</span><br><span class="line">start_time&#x3D;datetime.datetime.now()</span><br><span class="line"></span><br><span class="line">model.fit(x&#x3D;x_train,</span><br><span class="line">          y&#x3D;y_train,</span><br><span class="line">          batch_size&#x3D;batch_size,</span><br><span class="line">          epochs&#x3D;num_epochs)</span><br><span class="line">end_time&#x3D;datetime.datetime.now()</span><br><span class="line">time_cost&#x3D;end_time-start_time</span><br><span class="line">print(&quot;time_cost &#x3D; &quot;,time_cost) #CPU time cost: 5min, GPU time cost: less than 1min</span><br><span class="line"></span><br><span class="line">#模型保存</span><br><span class="line">model.save(&#39;E:&#x2F;ML&#x2F;LeNet_Model.hui&#39;)</span><br><span class="line"></span><br><span class="line">#评估指标</span><br><span class="line">print(model.evaluate(x_test, y_test))  #loss value &amp; metrics value</span><br><span class="line"></span><br><span class="line">#预测</span><br><span class="line">image_index&#x3D;4444</span><br><span class="line">print(x_test[image_index].shape)</span><br><span class="line">plt.imshow(x_test[image_index].reshape(32,32),cmap&#x3D;&#39;Greys&#39;)</span><br><span class="line"></span><br><span class="line">pred &#x3D; model.predict((x_test[image_index].reshape(1,32,32,1)))</span><br><span class="line">print(pred.argmax())</span><br></pre></td></tr></table></figure>
<h4 id="神经网络中的几个重要概念"><a href="#神经网络中的几个重要概念" class="headerlink" title="神经网络中的几个重要概念"></a>神经网络中的几个重要概念</h4><p><strong>梯度下降</strong>：梯度下降是一个在机器学习中用于寻找较佳结果（曲线的最小值）的迭代优化算法。梯度的含义是斜率或者斜坡的倾斜度。下降的含义是代价函数的下降。<br>&emsp;算法是迭代的，意思是需要多次使用算法获取结果，以得到最优化结果。梯度下降的迭代性质能使欠拟合演变成获得对数据的较佳拟合。<br><img src="/images/5868203-836b37ec01897d80.png" alt=""></p>
<p>&emsp;梯度下降中有一个称为学习率的参量。如上图左所示，刚开始学习率较大，因此下降步长更大。随着点的下降，学习率变得越来越小，从而下降步长也变小。同时，代价函数也在减小，或者说代价在减小，有时候也称为损失函数或者损失，两者是一样的。（损失/代价的减小是一个概念）。</p>
<p>&emsp;只有在数据很庞大的时候（在机器学习中，数据一般情况下都会很大），我们才需要使用epochs，batch size，iteration这些术语，在这种情况下，一次性将数据输入计算机是不可能的。因此，为了解决这个问题，我们需要把数据分成小块，一块一块的传递给计算机，在每一步的末端更新神经网络的权重，拟合给定的数据。</p>
<p>（1）batchsize：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；<br>（2）iteration：1个iteration等于使用batchsize个样本训练一次；<br>（3）epoch：1个epoch等于使用训练集中的全部样本训练一次；</p>
<blockquote>
<ul>
<li><strong>batchsize：批大小（批尺寸）</strong><br>批大小将决定我们一次训练的样本数目。<br>batch_size将影响到模型的优化程度和速度。</li>
</ul>
<p><strong>batchsize的正确选择是为了在内存效率和内存容量之间寻找最佳平衡。</strong><br>Batch_Size的取值：<br><img src="/images/5868203-d9e852a210bbf963.png" alt=""><br><strong>全批次（蓝色）</strong><br>如果数据集比较小，我们就采用全数据集。全数据集确定的方向能够更好的代表样本总体，从而更准确的朝向极值所在的方向。<br>注：对于大的数据集，我们不能使用全批次，因为会得到更差的结果。</p>
<p><strong>迷你批次（绿色）</strong><br>选择一个适中的Batch_Size值。就是说我们选定一个batch的大小后，将会以batch的大小将数据输入深度学习的网络中，然后计算这个batch的所有样本的平均损失，即代价函数是所有样本的平均。</p>
<p><strong>随机（Batch_Size等于1的情况）（红色）</strong><br>每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，难以达到收敛。</p>
<p>适当的增加Batch_Size的优点：</p>
<ul>
<li>1.通过并行化提高内存利用率。</li>
<li>2.单次epoch的迭代次数减少，提高运行速度。（单次epoch=(全部训练样本/batchsize)/iteration=1）</li>
<li>3.适当的增加Batch_Size,梯度下降方向准确度增加，训练震动的幅度减小。（看上图便可知晓）<h5 id="经验总结："><a href="#经验总结：" class="headerlink" title="经验总结："></a>经验总结：</h5>相对于正常数据集，如果Batch_Size过小，训练数据就会非常难收敛，从而导致underfitting。<br>增大Batch_Size,相对处理速度加快,所需内存容量增加（epoch的次数需要增加以达到最好的结果）<br>这里我们发现上面两个矛盾的问题，因为当epoch增加以后同样也会导致耗时增加从而速度下降。因此我们需要寻找最好的Batch_Size。</li>
</ul>
<p><strong>iteration：迭代</strong><br>迭代是重复反馈的动作，神经网络中我们希望通过迭代进行多次的训练以达到所需的目标或结果。<br>每一次迭代得到的结果都会被作为下一次迭代的初始值。<br>一个迭代=一个正向通过+一个反向通过</p>
<p><strong>Epoch</strong><br>当一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一次epoch。然而，当一个epoch对于计算机而言太庞大的时候，就需要把它分成多个小块。<br>为什么要使用多于一个epoch?<br>在神经网络中传递完整的数据集一次是不够的，而且我们需要将完整的数据集在同样的神经网络中传递多次。但请记住，我们使用的是有限的数据集，并且我们使用一个迭代过程即梯度下降来优化学习过程。如下图所示。因此仅仅更新一次或者说使用一个epoch是不够的。<br><img src="/images/5868203-c1ed21b3d4956665.png" alt=""><br>随着epoch数量增加，神经网络中的权重的更新次数也在增加，曲线从欠拟合变得过拟合。</p>
</blockquote>

      
      <!-- reward -->
      
      <div id="reward-btn">
        打赏
      </div>
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
          
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=http://ahuii.cn/2020/04/01/TensorFlow%E5%85%A5%E9%97%A8/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>
      
      

    </footer>

  </div>

  
  
  <nav class="article-nav">
    
      <a href="/2021/05/04/Docker-Gitlab-CI-Deploy/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            Docker&amp;Gitlab-CI Deploy
          
        </div>
      </a>
    
    
      <a href="/2020/03/07/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">目标检测算法总结</div>
      </a>
    
  </nav>


  

  

  
  
<div class="gitalk" id="gitalk-container"></div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.5.2/dist/gitalk.css">


<script src="https://cdn.jsdelivr.net/npm/gitalk@1.5.2/dist/gitalk.min.js"></script>


<script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script>

<script type="text/javascript">
  var gitalk = new Gitalk({
    clientID: '27f22c9606d2801d92eb',
    clientSecret: '629cbd4c87183101d7d8542013329b0c4fc04534',
    repo: 'AHU-HUI.github.io',
    owner: 'AHU-HUI',
    admin: ['AHU-HUI'],
    // id: location.pathname,      // Ensure uniqueness and length less than 50
    id: md5(location.pathname),
    distractionFreeMode: false,  // Facebook-like distraction free mode
    pagerDirection: 'last'
  })

  gitalk.render('gitalk-container')
</script>

  

</article>
</section>
      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2017-2021
        Shine
      </li>
      <li>
        
        Powered by
        
        
        <a href="https://hexo.io" target="_blank">Hexo</a> Theme <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <span>
  <i>PV:<span id="busuanzi_value_page_pv"></span></i>
  <i>UV:<span id="busuanzi_value_site_uv"></span></i>
</span>
        
      </li>
      
      <li>
        <a href="http://beian.miit.gov.cn/" target="_black">皖ICP备17013875号</a>
      </li>
      
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
      <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
    <aside class="sidebar">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="HUI&#39;s Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">旅行</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">摄影</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<script src="/js/share.js"></script>



<script src="/fancybox/jquery.fancybox.min.js"></script>




<script>
  try {
    var typed = new Typed("#subtitle", {
    strings: ['面朝大海，春暖花开！','愿你一生努力，一生被爱！','想要的都拥有，得不到的都释怀！'],
    startDelay: 0,
    typeSpeed: 500,
    loop: true,
    backSpeed: 200,
    showCursor: true
    });
  } catch (err) {
  }
  
</script>




<script src="/js/tocbot.min.js"></script>

<script>
  // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer:'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto',
    onClick: (e) => {
      $('.toc-link').removeClass('is-active-link');
      $(`a[href=${e.target.hash}]`).addClass('is-active-link');
      $(e.target.hash).scrollIntoView();
      return false;
    }
  });
</script>


<script>
  var ayerConfig = {
    mathjax: true
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>



<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>

    
  </div>
</body>

</html>